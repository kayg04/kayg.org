- When there's a failed drive/OSD, there are two situations:
    - The drive is dying but still has data available.
    - The disk is dead and no data can be retrieved from it.
- In both cases, the disk can be marked out with `ceph osd out <osd.num>` or `ceph osd reweight 0` (Both are equivalent operations).
- Ceph drains the dying OSD and moves data OR if the disk is dead, ceph rebuilds data from redundant bits / parity to _other OSDs on the same node_.
    - This might cause a [[Nearfull OSD]] situation.
    - The trick here is to set `ceph osd crush reweight 0`. This makes sure that the data is distributed to _other OSDs on all the nodes / throughout the crushmap_.
- Once the OSD has been drained (0 PGs), the disk can be replaced and the OSD can be destroyed and recreated.
- If the disk is dead, it can be immediately replaced and the OSD can be destroyed and recreated thereafter.